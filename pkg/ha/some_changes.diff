diff --git a/pkg/ha/client/client.go b/pkg/ha/client/client.go
index 2e7ea7d..f2b8ff6 100644
--- a/pkg/ha/client/client.go
+++ b/pkg/ha/client/client.go
@@ -10,6 +10,7 @@ import (
 	"time"
 
 	"github.com/jackc/pgconn"
+	"github.com/timescale/promscale/pkg/ha/state"
 	"github.com/timescale/promscale/pkg/pgmodel/common/schema"
 	"github.com/timescale/promscale/pkg/pgxconn"
 )
@@ -24,45 +25,18 @@ const (
 	leaderChangedErrCode = "PS010"
 )
 
-// LeaseDBState represents the current lock holder
-// as reported from the DB.
-type LeaseDBState struct {
-	Cluster    string
-	Leader     string
-	LeaseStart time.Time
-	LeaseUntil time.Time
-}
-
-// LeaseClient defines an interface for checking and changing leader status
-type LeaseClient interface {
-	// updateLease confirms permissions for a given leader wanting to insert
-	// data in a given time range.
-	// returns:
-	//   *haLockState current state of the lock
-	//   error - either a generic error that signifies
-	//		the check couldn't be performed
-	//		or a leaderHasChanged error signifying the leader has changed and HAState
-	//		needs to be updated
-	UpdateLease(ctx context.Context, cluster, replica string, minTime, maxTime time.Time) (*LeaseDBState, error)
-	// tryChangeLeader tries to set a new leader for a cluster
-	// returns:
-	// *haLockState current state of the lock (if try was successful state.leader == newLeader)
-	// error signifying the call couldn't be made
-	TryChangeLeader(ctx context.Context, cluster, newLeader string, maxTime time.Time) (*LeaseDBState, error)
-}
-
 type haLeaseClientDB struct {
 	dbConn pgxconn.PgxConn
 }
 
-func NewHaLeaseClient(dbConn pgxconn.PgxConn) LeaseClient {
+func NewHaLeaseClient(dbConn pgxconn.PgxConn) *haLeaseClientDB {
 	return &haLeaseClientDB{dbConn: dbConn}
 }
 
 func (h *haLeaseClientDB) UpdateLease(ctx context.Context, cluster, leader string, minTime, maxTime time.Time) (
-	dbState *LeaseDBState, err error,
+	dbState *state.LeaseState, err error,
 ) {
-	dbState = new(LeaseDBState)
+	dbState = new(state.LeaseState)
 	row := h.dbConn.QueryRow(ctx, updateLeaseSql, cluster, leader, minTime, maxTime)
 	leaderHasChanged := false
 	if err := row.Scan(&(dbState.Cluster), &(dbState.Leader), &(dbState.LeaseStart), &(dbState.LeaseUntil)); err != nil {
@@ -85,8 +59,8 @@ func (h *haLeaseClientDB) UpdateLease(ctx context.Context, cluster, leader strin
 	return
 }
 
-func (h *haLeaseClientDB) TryChangeLeader(ctx context.Context, cluster, newLeader string, maxTime time.Time) (*LeaseDBState, error) {
-	dbLock := LeaseDBState{}
+func (h *haLeaseClientDB) TryChangeLeader(ctx context.Context, cluster, newLeader string, maxTime time.Time) (*state.LeaseState, error) {
+	dbLock := state.LeaseState{}
 	row := h.dbConn.QueryRow(ctx, tryChangeLeaderSql, cluster, newLeader, maxTime)
 	if err := row.Scan(&(dbLock.Cluster), &(dbLock.Leader), &(dbLock.LeaseStart), &(dbLock.LeaseUntil)); err != nil {
 		return nil, err
@@ -94,8 +68,8 @@ func (h *haLeaseClientDB) TryChangeLeader(ctx context.Context, cluster, newLeade
 	return &dbLock, nil
 }
 
-func (h *haLeaseClientDB) readLeaseState(ctx context.Context, cluster string) (*LeaseDBState, error) {
-	dbLock := LeaseDBState{Cluster: cluster}
+func (h *haLeaseClientDB) readLeaseState(ctx context.Context, cluster string) (*state.LeaseState, error) {
+	dbLock := state.LeaseState{Cluster: cluster}
 	row := h.dbConn.QueryRow(ctx, latestLeaseStateSql, cluster)
 	if err := row.Scan(&dbLock.Leader, &dbLock.LeaseStart, &dbLock.LeaseUntil); err != nil {
 		return nil, err
diff --git a/pkg/ha/ha_parser_test.go b/pkg/ha/ha_parser_test.go
index 86a44d7..f34ece1 100644
--- a/pkg/ha/ha_parser_test.go
+++ b/pkg/ha/ha_parser_test.go
@@ -6,12 +6,13 @@ package ha
 
 import (
 	"fmt"
-	"github.com/timescale/promscale/pkg/pgmodel/cache"
 	"reflect"
 	"testing"
 	"time"
 
-	"github.com/timescale/promscale/pkg/ha/client"
+	"github.com/timescale/promscale/pkg/ha/state"
+	"github.com/timescale/promscale/pkg/pgmodel/cache"
+
 	"github.com/timescale/promscale/pkg/pgmodel/model"
 	"github.com/timescale/promscale/pkg/prompb"
 )
@@ -48,7 +49,7 @@ func TestHaParserParseData(t *testing.T) {
 		wantErr         bool
 		error           error
 		cluster         string
-		setClusterState *client.LeaseDBState
+		setClusterState *state.LeaseState
 	}{
 		{
 			name:   "Test: HA enabled but __replica__ && cluster are empty.",
@@ -128,7 +129,7 @@ func TestHaParserParseData(t *testing.T) {
 				}}},
 			wantNumRows: 1,
 			cluster:     "cluster1",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster1",
 				Leader:     "replica1",
 				LeaseStart: leaseStart,
@@ -154,7 +155,7 @@ func TestHaParserParseData(t *testing.T) {
 			wanted:      nil,
 			wantNumRows: 0,
 			cluster:     "cluster2",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster2",
 				Leader:     "replica1",
 				LeaseStart: leaseStart,
@@ -192,7 +193,7 @@ func TestHaParserParseData(t *testing.T) {
 			},
 			wantNumRows: 1,
 			cluster:     "cluster3",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster3",
 				Leader:     "replica1",
 				LeaseStart: leaseStart,
@@ -234,7 +235,7 @@ func TestHaParserParseData(t *testing.T) {
 			},
 			wantNumRows: 1,
 			cluster:     "cluster3",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster3",
 				Leader:     "replica1",
 				LeaseStart: leaseStart,
@@ -275,7 +276,7 @@ func TestHaParserParseData(t *testing.T) {
 			},
 			wantNumRows: 1,
 			cluster:     "cluster3",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster3",
 				Leader:     "replica1",
 				LeaseStart: leaseStart,
@@ -316,7 +317,7 @@ func TestHaParserParseData(t *testing.T) {
 			},
 			wantNumRows: 1,
 			cluster:     "cluster4",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster4",
 				Leader:     "replica2",
 				LeaseStart: leaseStart,
@@ -342,7 +343,7 @@ func TestHaParserParseData(t *testing.T) {
 			wanted:      map[string][]wanted{},
 			wantNumRows: 0,
 			cluster:     "cluster5",
-			setClusterState: &client.LeaseDBState{
+			setClusterState: &state.LeaseState{
 				Cluster:    "cluster5",
 				Leader:     "replica2",
 				LeaseStart: leaseStart,
diff --git a/pkg/ha/ha_service.go b/pkg/ha/ha_service.go
index e1ae22e..df5eeb1 100644
--- a/pkg/ha/ha_service.go
+++ b/pkg/ha/ha_service.go
@@ -9,7 +9,6 @@ import (
 	"sync"
 	"time"
 
-	"github.com/timescale/promscale/pkg/ha/client"
 	"github.com/timescale/promscale/pkg/ha/state"
 	"github.com/timescale/promscale/pkg/log"
 	"github.com/timescale/promscale/pkg/util"
@@ -43,7 +42,7 @@ const (
 // up to date by periodically refreshing it from the database.
 type Service struct {
 	state               *sync.Map
-	leaseClient         client.LeaseClient
+	leaseClient         state.LeaseClient
 	syncTicker          util.Ticker
 	currentTimeProvider func() time.Time
 	doneChannel         chan bool
@@ -54,7 +53,7 @@ type Service struct {
 // database connection and the default sync interval.
 // The sync interval determines how often the leases for
 // all clusters are refreshed from the database.
-func NewHAService(leaseClient client.LeaseClient) *Service {
+func NewHAService(leaseClient state.LeaseClient) *Service {
 	return NewHAServiceWith(leaseClient, util.NewTicker(haSyncerTimeInterval), time.Now)
 }
 
@@ -63,7 +62,7 @@ func NewHAService(leaseClient client.LeaseClient) *Service {
 // The ticker determines when the lease states for all clusters
 // are refreshed from the database, the currentTimeFn determines the
 // current time, used for deterministic tests.
-func NewHAServiceWith(leaseClient client.LeaseClient, ticker util.Ticker, currentTimeFn func() time.Time) *Service {
+func NewHAServiceWith(leaseClient state.LeaseClient, ticker util.Ticker, currentTimeFn func() time.Time) *Service {
 
 	service := &Service{
 		state:               &sync.Map{},
@@ -90,13 +89,7 @@ func (s *Service) haStateSyncer() {
 			s.state.Range(func(c, l interface{}) bool {
 				cluster := fmt.Sprint(c)
 				lease := l.(*state.Lease)
-				stateBeforeUpdate := lease.Clone()
-				stateAfterUpdate, err := lease.UpdateLease(
-					s.leaseClient,
-					stateBeforeUpdate.Leader,
-					stateBeforeUpdate.LeaseStart,
-					stateBeforeUpdate.MaxTimeSeenLeader,
-				)
+				stateAfterUpdate, err := lease.RefreshLease()
 				if err != nil {
 					errMsg := fmt.Sprintf(failedToUpdateLeaseErrFmt, cluster)
 					log.Error("msg", errMsg, "err", err)
@@ -104,7 +97,7 @@ func (s *Service) haStateSyncer() {
 				}
 
 				if s.shouldTryToChangeLeader(stateAfterUpdate) {
-					if _, err := lease.TryChangeLeader(s.leaseClient); err != nil {
+					if _, err := lease.TryChangeLeader(); err != nil {
 						errMsg := fmt.Sprintf(tryLeaderChangeErrFmt, cluster)
 						log.Error("msg", errMsg, "err", err)
 					}
@@ -134,7 +127,7 @@ func (s *Service) CheckLease(minT, maxT time.Time, clusterName, replicaName stri
 	case deny:
 		return false, time.Time{}, err
 	case doSync:
-		leaseState, err = lease.UpdateLease(s.leaseClient, replicaName, minT, maxT)
+		leaseState, err = lease.UpdateLease(replicaName, minT, maxT)
 		if err != nil {
 			errMsg := fmt.Sprintf(failedToUpdateLeaseErrFmt, clusterName)
 			log.Error("msg", errMsg, "err", err)
@@ -145,7 +138,7 @@ func (s *Service) CheckLease(minT, maxT time.Time, clusterName, replicaName stri
 			return false, time.Time{}, nil
 		}
 	case tryChangeLeader:
-		leaseState, err = lease.TryChangeLeader(s.leaseClient)
+		leaseState, err = lease.TryChangeLeader()
 		if err != nil {
 			errMsg := fmt.Sprintf(tryLeaderChangeErrFmt, clusterName)
 			log.Error("msg", errMsg, "err", err)
diff --git a/pkg/ha/mock_ha_lock_client.go b/pkg/ha/mock_ha_lock_client.go
index 50c1668..a05b9d1 100644
--- a/pkg/ha/mock_ha_lock_client.go
+++ b/pkg/ha/mock_ha_lock_client.go
@@ -9,17 +9,17 @@ import (
 	"fmt"
 	"time"
 
-	"github.com/timescale/promscale/pkg/ha/client"
+	"github.com/timescale/promscale/pkg/ha/state"
 )
 
 type mockLockClient struct {
-	leadersPerCluster map[string]*client.LeaseDBState
+	leadersPerCluster map[string]*state.LeaseState
 }
 
-func (m *mockLockClient) UpdateLease(_ context.Context, cluster, leader string, minTime, maxTime time.Time) (*client.LeaseDBState, error) {
+func (m *mockLockClient) UpdateLease(_ context.Context, cluster, leader string, minTime, maxTime time.Time) (*state.LeaseState, error) {
 	lock, exists := m.leadersPerCluster[cluster]
 	if !exists {
-		lock = &client.LeaseDBState{
+		lock = &state.LeaseState{
 			Cluster:    cluster,
 			Leader:     leader,
 			LeaseStart: minTime,
@@ -31,12 +31,12 @@ func (m *mockLockClient) UpdateLease(_ context.Context, cluster, leader string,
 	return lock, nil
 }
 
-func (m *mockLockClient) TryChangeLeader(_ context.Context, cluster, newLeader string, maxTime time.Time) (*client.LeaseDBState, error) {
+func (m *mockLockClient) TryChangeLeader(_ context.Context, cluster, newLeader string, maxTime time.Time) (*state.LeaseState, error) {
 	lock, exists := m.leadersPerCluster[cluster]
 	if !exists {
 		return nil, fmt.Errorf("no leader for %s, UpdateLease never called before TryChangeLeader", cluster)
 	}
-	lock = &client.LeaseDBState{
+	lock = &state.LeaseState{
 		Cluster:    cluster,
 		Leader:     newLeader,
 		LeaseStart: lock.LeaseUntil,
@@ -47,5 +47,5 @@ func (m *mockLockClient) TryChangeLeader(_ context.Context, cluster, newLeader s
 }
 
 func newMockLockClient() *mockLockClient {
-	return &mockLockClient{leadersPerCluster: make(map[string]*client.LeaseDBState)}
+	return &mockLockClient{leadersPerCluster: make(map[string]*state.LeaseState)}
 }
diff --git a/pkg/ha/mock_ha_service.go b/pkg/ha/mock_ha_service.go
index a3026d5..ad82481 100644
--- a/pkg/ha/mock_ha_service.go
+++ b/pkg/ha/mock_ha_service.go
@@ -8,10 +8,10 @@ import (
 	"sync"
 	"time"
 
-	"github.com/timescale/promscale/pkg/ha/client"
+	"github.com/timescale/promscale/pkg/ha/state"
 )
 
-func MockNewHAService(clusterInfo []*client.LeaseDBState) *Service {
+func MockNewHAService(clusterInfo []*state.LeaseState) *Service {
 	lockClient := newMockLockClient()
 
 	for _, c := range clusterInfo {
@@ -27,7 +27,7 @@ func MockNewHAService(clusterInfo []*client.LeaseDBState) *Service {
 }
 
 func SetLeaderInMockService(service *Service, cluster, leader string, minT, maxT time.Time) {
-	service.leaseClient.(*mockLockClient).leadersPerCluster[cluster] = &client.LeaseDBState{
+	service.leaseClient.(*mockLockClient).leadersPerCluster[cluster] = &state.LeaseState{
 		Cluster:    cluster,
 		Leader:     leader,
 		LeaseStart: minT,
diff --git a/pkg/ha/state/lease.go b/pkg/ha/state/lease.go
index b2515eb..76ccb52 100644
--- a/pkg/ha/state/lease.go
+++ b/pkg/ha/state/lease.go
@@ -6,7 +6,6 @@ import (
 	"sync"
 	"time"
 
-	"github.com/timescale/promscale/pkg/ha/client"
 	"github.com/timescale/promscale/pkg/pgmodel/metrics"
 )
 
@@ -19,15 +18,9 @@ import (
 // and the real time when the leader last sent data.
 // 	Access to any field should be controlled through the _mu RW Mutex.
 type Lease struct {
-	cluster               string
-	leader                string
-	leaseStart            time.Time
-	leaseUntil            time.Time
-	maxTimeSeen           time.Time // max data time seen from any prometheus replica
-	maxTimeInstance       string    // the replica name that’s seen the maxtime
-	maxTimeSeenLeader     time.Time // max data time seen by current leader
-	recentLeaderWriteTime time.Time // real time when leader last wrote data
-	_mu                   sync.RWMutex
+	client LeaseClient
+	state  LeaseState
+	_mu    sync.RWMutex
 }
 
 // LeaseState represents a snapshot of a lease
@@ -39,10 +32,10 @@ type LeaseState struct {
 	Leader                string
 	LeaseStart            time.Time
 	LeaseUntil            time.Time
-	MaxTimeSeen           time.Time
-	MaxTimeInstance       string
-	MaxTimeSeenLeader     time.Time
-	RecentLeaderWriteTime time.Time
+	MaxTimeSeen           time.Time // max data time seen from any prometheus replica
+	MaxTimeInstance       string    // the replica name that’s seen the maxtime
+	MaxTimeSeenLeader     time.Time // max data time seen by current leader
+	RecentLeaderWriteTime time.Time // real time when leader last wrote data
 }
 
 // Creates a new Lease and immediately synchronizes with the database, it either
@@ -51,39 +44,41 @@ type LeaseState struct {
 //	- or the existing leader and lease details are returned and set in the
 //	  new Lease.
 // An error is returned if an error occurred querying the database.
-func NewLease(c client.LeaseClient, cluster, potentialLeader string, minT, maxT, currentTime time.Time) (*Lease, error) {
+func NewLease(c LeaseClient, cluster, potentialLeader string, minT, maxT, currentTime time.Time) (*Lease, error) {
 	dbState, err := c.UpdateLease(context.Background(), cluster, potentialLeader, minT, maxT)
 	if err != nil {
 		return nil, fmt.Errorf("could not create new lease: %#v", err)
 	}
+	dbState.MaxTimeSeen = maxT
+	dbState.MaxTimeInstance = potentialLeader
+	dbState.MaxTimeSeenLeader = maxT
+	dbState.RecentLeaderWriteTime = currentTime
 
 	return &Lease{
-		cluster:               dbState.Cluster,
-		leader:                dbState.Leader,
-		leaseStart:            dbState.LeaseStart,
-		leaseUntil:            dbState.LeaseUntil,
-		maxTimeSeen:           maxT,
-		maxTimeInstance:       potentialLeader,
-		maxTimeSeenLeader:     maxT,
-		recentLeaderWriteTime: currentTime,
+		client: c,
+		state:  *dbState,
 	}, nil
 }
 
+func (h *Lease) RefreshLease() (LeaseState, error) {
+	return h.UpdateLease(h.state.Leader, h.state.LeaseStart, h.state.MaxTimeSeenLeader)
+}
+
 // UpdateLease uses the supplied client to attempt to update the lease for the potentialLeader.
 // It either updates the lease, or a new leader with the assigned lease interval is set, as
 // signified by the database as the source of truth.
 // An error is returned if the db can't be reached.
-func (h *Lease) UpdateLease(c client.LeaseClient, potentialLeader string, minT, maxT time.Time) (*LeaseState, error) {
+func (h *Lease) UpdateLease(potentialLeader string, minT, maxT time.Time) (LeaseState, error) {
 	h._mu.RLock()
-	cluster := h.cluster
-	if h.leader != potentialLeader {
+	cluster := h.state.Cluster
+	if h.state.Leader != potentialLeader {
 		h._mu.RUnlock()
-		return nil, fmt.Errorf("should never be updating the lease for a non-leader")
+		return LeaseState{}, fmt.Errorf("should never be updating the lease for a non-leader")
 	}
 	h._mu.RUnlock()
-	stateFromDB, err := c.UpdateLease(context.Background(), cluster, potentialLeader, minT, maxT)
+	stateFromDB, err := h.client.UpdateLease(context.Background(), cluster, potentialLeader, minT, maxT)
 	if err != nil {
-		return nil, fmt.Errorf("could not update lease from db: %#v", err)
+		return LeaseState{}, fmt.Errorf("could not update lease from db: %#v", err)
 	}
 	return h.setUpdateFromDB(stateFromDB), nil
 }
@@ -92,18 +87,18 @@ func (h *Lease) UpdateLease(c client.LeaseClient, potentialLeader string, minT,
 // of the cluster based on the maximum observed data time and the instance
 // that had it. If updates the lease with the latest state from the database.
 // An error is returned if the db can't be reached.
-func (h *Lease) TryChangeLeader(c client.LeaseClient) (*LeaseState, error) {
+func (h *Lease) TryChangeLeader() (LeaseState, error) {
 	h._mu.RLock()
-	cluster := h.cluster
-	maxTimeInstance := h.maxTimeInstance
-	maxTimeSeen := h.maxTimeSeen
+	cluster := h.state.Cluster
+	maxTimeInstance := h.state.MaxTimeInstance
+	maxTimeSeen := h.state.MaxTimeSeen
 	h._mu.RUnlock()
-	leaseState, err := c.TryChangeLeader(
+	leaseState, err := h.client.TryChangeLeader(
 		context.Background(), cluster, maxTimeInstance, maxTimeSeen,
 	)
 
 	if err != nil {
-		return nil, fmt.Errorf("could not call try change leader from db: %#v", err)
+		return LeaseState{}, fmt.Errorf("could not call try change leader from db: %#v", err)
 	}
 
 	return h.setUpdateFromDB(leaseState), nil
@@ -115,19 +110,19 @@ func (h *Lease) TryChangeLeader(c client.LeaseClient) (*LeaseState, error) {
 func (h *Lease) UpdateMaxSeenTime(currentReplica string, currentMaxT, currentWallTime time.Time) {
 	h._mu.Lock()
 	defer h._mu.Unlock()
-	if currentMaxT.After(h.maxTimeSeen) {
-		h.maxTimeInstance = currentReplica
-		h.maxTimeSeen = currentMaxT
+	if currentMaxT.After(h.state.MaxTimeSeen) {
+		h.state.MaxTimeInstance = currentReplica
+		h.state.MaxTimeSeen = currentMaxT
 	}
 
-	if currentReplica != h.leader {
+	if currentReplica != h.state.Leader {
 		return
 	}
 
-	if currentMaxT.After(h.maxTimeSeenLeader) {
-		h.maxTimeSeenLeader = currentMaxT
+	if currentMaxT.After(h.state.MaxTimeSeenLeader) {
+		h.state.MaxTimeSeenLeader = currentMaxT
 	}
-	h.recentLeaderWriteTime = currentWallTime
+	h.state.RecentLeaderWriteTime = currentWallTime
 }
 
 // GetLeader returns the current leader. To be used
@@ -136,44 +131,44 @@ func (h *Lease) UpdateMaxSeenTime(currentReplica string, currentMaxT, currentWal
 func (h *Lease) GetLeader() string {
 	h._mu.RLock()
 	defer h._mu.RUnlock()
-	return h.leader
+	return h.state.Leader
+}
+
+func (h *Lease) GetLeaseDeadline() time.Time {
+	h._mu.RLock()
+	defer h._mu.RUnlock()
+	return h.state.LeaseUntil
+}
+
+func (h *Lease) GetRecentLeaderWriteTime() time.Time {
+	h._mu.RLock()
+	defer h._mu.RUnlock()
+	return h.state.RecentLeaderWriteTime
 }
 
 // Safely creates a LeaseView form the current lease state.
 // The LeaseView is to be used locally in a single goroutine
 // to avoid the need for a RLock each time one of the fields
 // needs to be read.
-func (h *Lease) Clone() *LeaseState {
+func (h *Lease) Clone() LeaseState {
 	h._mu.RLock()
 	defer h._mu.RUnlock()
-	return h.locklessClone()
-}
-func (h *Lease) locklessClone() *LeaseState {
-	return &LeaseState{
-		Cluster:               h.cluster,
-		Leader:                h.leader,
-		LeaseStart:            h.leaseStart,
-		LeaseUntil:            h.leaseUntil,
-		MaxTimeSeen:           h.maxTimeSeen,
-		MaxTimeInstance:       h.maxTimeInstance,
-		MaxTimeSeenLeader:     h.maxTimeSeenLeader,
-		RecentLeaderWriteTime: h.recentLeaderWriteTime,
-	}
+	return h.state
 }
 
-func (h *Lease) setUpdateFromDB(stateFromDB *client.LeaseDBState) *LeaseState {
+func (h *Lease) setUpdateFromDB(stateFromDB *LeaseState) LeaseState {
 	h._mu.Lock()
 	defer h._mu.Unlock()
-	oldLeader := h.leader
-	h.leader = stateFromDB.Leader
-	h.leaseStart = stateFromDB.LeaseStart
-	h.leaseUntil = stateFromDB.LeaseUntil
-	if oldLeader != h.leader {
-		h.maxTimeSeenLeader = time.Time{}
-		h.recentLeaderWriteTime = time.Now()
+	oldLeader := h.state.Leader
+	h.state.Leader = stateFromDB.Leader
+	h.state.LeaseStart = stateFromDB.LeaseStart
+	h.state.LeaseUntil = stateFromDB.LeaseUntil
+	if oldLeader != h.state.Leader {
+		h.state.MaxTimeSeenLeader = time.Time{}
+		h.state.RecentLeaderWriteTime = time.Now()
 	}
 	exposeHAStateToMetrics(stateFromDB.Cluster, oldLeader, stateFromDB.Leader)
-	return h.locklessClone()
+	return h.state
 }
 
 func exposeHAStateToMetrics(cluster, oldLeader, newLeader string) {
